---
title: "greedyAlgorithmFeatureSelection"
author: "JuMingXing"
date: "2016年9月8日"
output: html_document
---
贪心算法（又称贪婪算法）是指，在对问题求解时，总是做出在当前看来是最好的选择。也就是说，不从整体最优上加以考虑，它所做出的是在某种意义上的局部最优解。贪心算法不是对所有问题都能得到整体最优解，关键是贪心策略的选择，选择的贪心策略必须具备无后效性，即某个状态以前的过程不会影响以后的状态，只与当前状态有关。

算法设计：

  1. 初始化问题的目标值
  
  2. while（实现优化目标的约束条件）{
  
      利用筛选策略，求出解空间的一个可行解
      
    }
    
  3. 将所有可行解组合成目标解空间。
```{r}
options(warn = -1)
require(magrittr)
require(dplyr)
require(glmnet)
# Greedy Algorithm
greedyAlgorithm = function(dataSet) {
  features = data.frame(name = colnames(dataSet)) %>%
    dplyr::filter(name != "label") 
  features = as.vector(features$name)  
  featureSelect = c("label")
  scoreBefore = data.frame()
  while((nrow(scoreBefore)<2||scoreBefore[length(scoreBefore),2]>
        scoreBefore[length(scoreBefore) - 1,2])&&nrow(scoreBefore)<length(features)){
    score = data.frame()
    for(feature in features){
      if(length(intersect(feature,featureSelect)) == 0){
        trainData = dataSet[,append(featureSelect,feature)]
        model = glm(label~.,family = "binomial",data = trainData,epsilon = 1e-10)
        prediction = predict(model,trainData)
        aucValue = auc(trainData$label,prediction)
        score = rbind(score,data.frame(feature = feature,aucValue = aucValue))
      }
    }
    featureSelect = unique(append(featureSelect,as.character(score[which.max(score$aucValue),1])))
    print(length(featureSelect))
    scoreBefore = rbind(scoreBefore,score[which.max(score$aucValue),])
  }
  featureSelect = head(featureSelect,length(featureSelect)-1)
  return(featureSelect[-1])
}
```
KS值表征了模型将正例和负例区分开来的能力。值越大，模型的预测准确性越好。通常情况下，KS>0.3即可认为模型有比较好的预测准确性。

KS值计算方法：

将所有样本根据预测得分从低到高排序均分成N组，分别计算这N组的实际好样本数、坏样本数、累积好样本数、累积坏样本数、累积好样本数占比、累积坏样本数占比，差值。其中，实际好坏样本数分别为该组内的好坏样本数，累积好坏样本数为该组累积的好坏样本数，累积好坏样本数占比为累积好坏样本数占总好坏样本数的比值，差值为累积坏样本数占比减去累计好样本数占比。KS指标为差值绝对值的最大值。
```{r}
# ksValue
ksValue = function(prediction,n){
  dataResult = arrange(desc(prediction))
  a = c()
  b = c()
  c = c()
  a[1] = 0
  b[1] = 0
  c[1] = 0
  if(nrow(dataResult)%%n==0){
    cut = nrow(dataResult)/n
    for (i in 2:(n+1)) {
      a[i] = sum(dataResult[(cut*(i-2)+1):(cut*(i-1)),1])
      b[i] = nrow(dataResult[(cut*(i-2)+1):(cut*(i-1)),])-a[i]
    }
  }else{
    cut = round(nrow(dataResult)/n)
    for (i in 2:n) {
      a[i] = sum(dataResult[(cut*(i-2)+1):(cut*(i-1)),1])
      b[i] = nrow(dataResult[(cut*(i-2)+1):(cut*(i-1)),])-a[i]
    }
    a[n+1] = sum(dataResult[(cut*(n-2)+1):(cut*(n-1)),1])
    b[n+1] = nrow(dataResult[(cut*(n-2)+1):(cut*(n-1)),])-a[n+1]
  }
  c = abs(cumsum(a)/sum(a)-cumsum(b)/sum(b))
  return(c)
}
```

```{r}
require(caret)
# dataSet = data.frame(matrix(rnorm(10000),100,100))
# colName = paste("a",1:100,sep = "")
# colnames(dataSet) = colName
# dataSet %<>%
#   dplyr::mutate(label = c(rep(1,46),rep(0,54)))
data = read.csv("/data/workspace/Rworkspace/model_risk_python/data_train_model.csv",encoding = "UTF-8")
data %<>%
  mutate(label = ifelse(overDueDays>30,1,0))
data = data[,-c(1,2)]
data = data.frame(apply(data, 2, function(x) ifelse(is.na(x),median(x,na.rm = T),x)))
# 剔除近似常量的变量
fea_1 = nearZeroVar(train_data_tmp)
train_data_tmp = train_data_tmp[,-fea_1]
# 剔除相关度过高的自变量
data_cor = cor(train_data_tmp)
highcor = findCorrelation(data_cor,0.8)
train_data_tmp = train_data_tmp[,-highcor]
feature = greedyAlgorithm(dataSet = data)
set.seed(521)
ind = base::sample(2,nrow(data),replace=T,prob=c(0.7,0.3))
trainData = data[ind==1,]
testData = data[ind==2,]
model = cv.glmnet(as.matrix(trainData[,feature]),trainData[,"label"],
                    family = "binomial",type.measure = "auc",alpha = 0,
                    lambda.min.ratio = 0.0001)
prediction = predict(model,as.matrix(testData[,feature]),s="lambda.min",type="response")
# compute ksValue
ksValue = ksValue(prediction,10)
par(mfrow = c(1,2))
plot(density(ksValue),type = 'l',main = "ksValue Plot",xlab = "cutPoint",ylab = "density_ks")
text(.2,1.5,paste("ksValue = ",max(ksValue)))
roc(testData$label, as.vector(prediction), auc = T,plot = T,print.auc=T)
par(mfrow=c(1,1))
```



